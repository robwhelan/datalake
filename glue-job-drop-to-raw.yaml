AWSTemplateFormatVersion: "2010-09-09"
Description: A job that moves data from the drop zone to the raw zone.
Parameters:
  pProjectName:
    Type: String
    Description: the project Name
  pDatabaseName:
    Type: String
    Description: the glue database created for this dataset
  pTableName:
    Type: String
    Description: the source table for this data.
  pDataPartition:
    Type: String
    Description: the partition for where the file will reside in S3 ... example, "orders"
  pDownstreamBucket:
    Type: String
    Description: the bucket downstream of hte drop zone bucket.
  pGlueJobRoleArn:
    Type: String
    Description: output from the main stack. A role that authorizes Glue to access S3.
  pScriptLocation:
    Type: String
    Description: the script location for the etl.
Resources:
  rGlueJobDropToRaw:
    Type: AWS::Glue::Job
    Properties:
      #AllocatedCapacity: Double
      GlueVersion: 1.0
      Command:
        Name: glueetl #default for apache spark jobs
        PythonVersion: 3
        ScriptLocation: !Ref pScriptLocation
      DefaultArguments:
        "--database_name": !Ref pDatabaseName
        '--table_name': !Ref pTableName
        "--job-bookmark-option": "job-bookmark-enable"
        "--data_partition": !Ref pDataPartition
        "--downstream_bucket": !Ref pDownstreamBucket
      Description: Convert incoming CSV to Parquet.
      Name: !Sub '${pProjectName}-glue-job-drop-to-raw-${pTableName}'
      Role: !Ref pGlueJobRoleArn
#TODO: create a trigger says when to run this job. By default, it should run daily.
